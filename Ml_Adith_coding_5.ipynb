{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 5 - Logistic Regression \n",
    "\n",
    "## In this assignment:\n",
    "\n",
    "You'll employ gradient ascent to determine weights for a logistic regression problem focused on diagnosing breast cancer.\n",
    "\n",
    "### Dataset Overview:\n",
    "\n",
    "The **Breast Cancer Wisconsin dataset** is a widely-recognized collection of features manually recorded by physicians from fine needle aspiration samples. The primary objective is to determine whether the cells are benign or malignant based on these features. \n",
    "\n",
    "**Dataset details:** [Breast Cancer Wisconsin dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin)\n",
    "\n",
    "Each sample from the dataset is derived from a digitized image of a fine needle aspirate (FNA) of a breast mass. These images are processed to extract characteristics of cell nuclei, which are instrumental in the diagnostic process.\n",
    "\n",
    "### Features:\n",
    "\n",
    "The dataset consists of ten real-valued features that provide various measurements related to the cell nucleus:\n",
    "\n",
    "1. **Radius:** Mean of distances from the center to points on the perimeter.\n",
    "2. **Texture:** Standard deviation of gray-scale values.\n",
    "3. **Perimeter**\n",
    "4. **Area**\n",
    "5. **Smoothness:** Local variation in radius lengths.\n",
    "6. **Compactness:** \\( \\frac{\\text{perimeter}^2}{\\text{area}} - 1.0 \\)\n",
    "7. **Concavity:** Severity of concave portions of the contour.\n",
    "8. **Concave Points:** Number of concave portions of the contour.\n",
    "9. **Symmetry**\n",
    "10. **Fractal Dimension:** \"Coastline approximation\" - 1.\n",
    "\n",
    "### Task:\n",
    "\n",
    "Your mission is to use logistic regression on the provided features to predict whether a tumor is benign or malignant. Successfully doing so can greatly aid in early diagnosis, ultimately leading to saved lives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, preprocessing, and understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the standard libraries\n",
    "\n",
    "### Essential Libraries\n",
    "\n",
    "- **NumPy**: A library for numerical operations in Python.\n",
    "- **Matplotlib**: Provides a way to visualize data.\n",
    "\n",
    "### Scikit-learn Utilities\n",
    "\n",
    "- **load_breast_cancer**: Dataset included in Scikit-learn for breast cancer classification.\n",
    "- **preprocessing**: Contains methods for preparing data before applying learning algorithms.\n",
    "- **train_test_split**: A utility function to split data into training and testing sets.\n",
    "\n",
    "> **Note**: Using the `%matplotlib inline` command ensures that Matplotlib visualizations are rendered directly within the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Set up matplotlib for inline display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "y = cancer.target\n",
    "X = cancer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of data (X) and target (Y) values \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "### Splitting the Data\n",
    "We divide our dataset into a training set and a testing set:\n",
    "- **Training Set**: 75%\n",
    "- **Testing Set**: 25%\n",
    "\n",
    "Use the `train_test_split` function to achieve this split:\n",
    "- Assign results to: `X_train`, `X_test`, `y_train`, `y_test`\n",
    "- Set `random_state` to 42 to ensure reproducibility.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data Using Standard Scaler\n",
    "\n",
    "Since we are using gradient ascent, it's important to scale our data to ensure faster convergence. One of the most common methods to scale data is to use the `Standard Scaler`.\n",
    "\n",
    "The `Standard Scaler` normalizes the features by subtracting the mean and scaling to unit variance. \n",
    "\n",
    "Using `Standard Scaler`, each feature will have a mean of 0 and a standard deviation of 1 post-scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Print the shape of x_train and y_train \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "##\n",
    " # When you print the shape of x_train, it should print (426, 30)\n",
    " # When you print the shape of y_train, it should print (426,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a Bias Term to $X_{\\text{train}}$ and $X_{\\text{test}}$\n",
    "To account for the intercept term in our logistic regression model, we augment our feature matrices with a column of ones. This is often referred to as the bias term.\n",
    "\n",
    "Given our original matrix:\n",
    "$$X_{\\text{train}}=\\left[\\begin{matrix}\n",
    "x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots \\\\\n",
    "x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "We add a column of ones:\n",
    "$$ X_{\\text{train}}=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "Similarly, we augment $X_{\\text{test}}$ with a column of ones. This allows our algorithm to learn an intercept term without needing special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainng data has dimensions:  (426, 31) . The testing data has dimensions:  (143, 31)\n",
      "[[ 1.         -0.34913849 -1.43851335 -0.41172595 -0.39047943 -1.86366229\n",
      "  -1.26860704 -0.82617052 -0.95286585 -1.72936805 -0.9415409  -0.86971355\n",
      "  -1.35865347 -0.83481506 -0.57230673 -0.74586846 -0.65398319 -0.52583524\n",
      "  -0.94677147 -0.53781728 -0.63449458 -0.54268486 -1.65565452 -0.58986401\n",
      "  -0.52555985 -1.51066925 -0.89149994 -0.75021715 -0.91671059 -0.92508585\n",
      "  -0.80841115]\n",
      " [ 1.         -0.20468665  0.31264011 -0.13367256 -0.27587995  1.07807258\n",
      "   0.86354605  0.72631375  0.89844062  1.17876963  1.47437716 -0.04022275\n",
      "  -0.50962253  0.10947722 -0.13472838 -0.52489487 -0.14934475  0.07460028\n",
      "   0.23747244 -0.43028253  0.08289146  0.04148684  0.68989862  0.19412774\n",
      "  -0.05193356  1.12941497  0.92394223  1.22221738  1.43655962  1.14955889\n",
      "   1.56911143]]\n"
     ]
    }
   ],
   "source": [
    "# Appending a column of ones to x_train \n",
    "\n",
    "# Step 1: Create a column vector of ones (i.e. a vector of shape N',1)\n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))\n",
    "\n",
    "# Step 2: Append a column of ones in the beginning of x_train\n",
    "X_train = np.hstack((ones, X_train))\n",
    "\n",
    "\n",
    "# Now do the same for the test data\n",
    "# Step 1: Create a column vector of ones (i.e. a vector of shape N\",1)\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "\n",
    "# Stemp 2: Append a column of ones in the beginning of x_test\n",
    "X_test = np.hstack((ones, X_test))\n",
    "\n",
    "\n",
    "# We can check that everything worked correctly by:\n",
    "# Printing out the new dimensions\n",
    "print(\"The trainng data has dimensions: \", X_train.shape, \". The testing data has dimensions: \",X_test.shape)\n",
    "\n",
    "# Looking at the first two rows of X_train to check everything worked as expected\n",
    "print(X_train[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Target Vectors\n",
    "\n",
    "due to the broadcasting feature in libraries like NumPy, if we're not careful with the shapes of our matrices, we might unintentionally compute the outer product instead of the desired inner product. This can lead to unexpected results and potential bugs in the algorithm. Reshaping the target vectors into 2D arrays helps prevent such issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape((y_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "# Read through the description of the dataset by uncommenting the line of code below\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
      "        1.189e-01],\n",
      "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
      "        8.902e-02],\n",
      "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
      "        8.758e-02],\n",
      "       ...,\n",
      "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
      "        7.820e-02],\n",
      "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
      "        1.240e-01],\n",
      "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
      "        7.039e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'frame': None, 'target_names': array(['malignant', 'benign'], dtype='<U9'), 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error',\n",
      "       'fractal dimension error', 'worst radius', 'worst texture',\n",
      "       'worst perimeter', 'worst area', 'worst smoothness',\n",
      "       'worst compactness', 'worst concavity', 'worst concave points',\n",
      "       'worst symmetry', 'worst fractal dimension'], dtype='<U23'), 'filename': 'breast_cancer.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "# You can add your own code here to better understand the dataset\n",
    "print(cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Fitting the model\n",
    "\n",
    "## Implementing Logistic Regression Using Gradient Ascent\n",
    "\n",
    "\n",
    "You will perform the following steps:\n",
    "* write the sigmoid function $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "* initialize ${\\bf w}$\n",
    "* prediction: write the function to compute the probability of every example in $X$ belonging to class one\n",
    "* write the log likelihood function (see lecture notes for the formula)\n",
    "* write the gradient ascent algorithm\n",
    "* plot the likelihood v/s the number of iterations\n",
    "* predict the class label (i.e. $0,1$) for every example in $X$ for a given ${\\bf w}$ and $t$\n",
    "* Evaluate your hypothesis by using your hypothesis to predict the label of the examples in the test set.  Using these predicted value you will then determine the precision, recall and F1 score of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "### Sigmoid($z$)\n",
    "The first function you will write is sigmoid($z$)\n",
    "\n",
    "sigmoid($z$) takes as input a column vector of real numbers, $z^T = [z_1, z_2, ..., z_{N'}]$, where $N'$ is the number of  examples\n",
    "\n",
    "It should produce as output a column vector $\\left[\\frac{1}{1+e^{-z_1}},\\frac{1}{1+e^{-z_2}},...,\\frac{1}{1+e^{-z_{N'}}}\\right]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write the sigmoid function (z can be a scalar or a vector)\n",
    "def sigmoid(z):\n",
    "    ## TODO\n",
    "\n",
    "    if(isinstance(z,int)):\n",
    "        l=np.power(np.e,-z)\n",
    "        k=1/(1+l)\n",
    "        return k\n",
    "\n",
    "    g=[]\n",
    "    \n",
    "    for i in range(len(z)):\n",
    "        k=1/(1+np.power(np.e,-z[i]))\n",
    "        g.append(k)\n",
    "    \n",
    "\n",
    "\n",
    "    g1=np.array(g)\n",
    "    return g1\n",
    "    ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# VERIFY - Sigmoid of 0 should be equal to 0.5\n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing ${\\bf w}$\n",
    "For testing the next functions, we create a coefficient vector, ${\\bf w}$.\n",
    "We will initialize the coeffients to be $0$, i.e. ${\\bf w}^T = [0,0,\\ldots ,0]$ (We could have initialized ${\\bf w}$ with any values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters w\n",
    "## TODO\n",
    "w =np.zeros(X_train.shape[1])\n",
    "##\n",
    "w=w.reshape(X_train.shape[1],1)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function\n",
    "Complete the `hypothesis` function to compute the probability that each example in \\(X\\) belongs to class one. Specifically, it calculates:\n",
    "\n",
    "$$\\hat{\\bf y}=\\sigma(X{\\bf w})$$\n",
    "\n",
    "For a single example represented by the design matrix:\n",
    "\n",
    "$$X=[1,x_1,x_2,\\ldots,x_d]$$\n",
    "\n",
    "and the corresponding weight vector:\n",
    "\n",
    "$${\\bf w}^T=[w_0,w_1,\\ldots, w_d]$$\n",
    "\n",
    "The function returns the logistic regression prediction:\n",
    "\n",
    "$$h({\\bf x})=\\frac{1}{1+e^{-\\left({w_{0}\\cdot 1 +w_1\\cdot x_1+\\cdots +w_d\\cdot x_d}\\right)}}$$\n",
    "\n",
    "Given a matrix with $N'$ examples:\n",
    "\n",
    "$$X=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "with the same weight vector, the function will return:\n",
    "\n",
    "$$[h({\\bf x}^{(1)}),h({\\bf x}^{(2)}),\\ldots, h({\\bf x}^{(N')})]^T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probability that a patient has cancer \n",
    "# TODO - Write the hypothesis function \n",
    "def hypothesis(X , w):\n",
    "    #TODO\n",
    "    hh=[]\n",
    "    ww=np.transpose(w)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        zz=np.dot(ww,X[i])\n",
    "        hh.append(zz)\n",
    "\n",
    "\n",
    "    g=sigmoid(hh)    \n",
    "    return g\n",
    "    ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, do a quick check that your function can accpet a matrix as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 1)\n",
      "(426, 1)\n"
     ]
    }
   ],
   "source": [
    "# Compute y_hat using our training examples and w (w is still set to zero).  \n",
    "# This is just a preliminary test of the hypotheis function\n",
    "\n",
    "\n",
    "yhat = hypothesis(X_train, w)\n",
    "\n",
    "# print the sizes of yhat and y as a first check that the function performed correctly\n",
    "\n",
    "print(yhat.shape) # this should return (426, 1)\n",
    "print(y_train.shape) # this should return (426,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood Function\n",
    "\n",
    "Write the function to calculate the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell({\\bf w})= \\sum_{i=1}^{N'} y^{(i)} \\ln(h({\\bf x}^{(i)})) + (1 - y^{(i)}) \\ln(1 - h({\\bf x}^{(i)}))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Input**:\n",
    "  - Design matrix with $N'$ examples:\n",
    "    \n",
    "   $$\n",
    "    X = \\left[\\begin{array}{cccc}\n",
    "    1 & x^{(1)}_1 & \\ldots & x^{(1)}_d \\\\\n",
    "    1 & x^{(2)}_1 & \\ldots & x^{(2)}_d \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x^{(N')}_1 & \\ldots & x^{(N')}_d \\\\\n",
    "    \\end{array}\\right]\n",
    "   $$\n",
    "    \n",
    "  - Column vector of labels for $X$:\n",
    "    \n",
    "  $$\n",
    "    {\\bf y}^T = [y^{(1)}, y^{(2)}, \\ldots, y^{(N')}]\n",
    "   $$\n",
    "  \n",
    "- **Output**:\n",
    "  - Log-likelihood value: $\\ell({\\bf w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write the log likelihood function \n",
    "def log_likelihood(X , y , w):\n",
    "    ##TODO\n",
    "    yh=hypothesis(X,w)\n",
    "    s=0\n",
    "\n",
    "\n",
    "    for i in range(len(yh)):\n",
    "        a=np.log(yh[i])\n",
    "        b=np.log(1-yh[i])\n",
    "        c=y[i]*a\n",
    "        d=(1-y[i])*b\n",
    "        s=s+c+d\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    log_likelihood=s\n",
    "    ##\n",
    "    return log_likelihood # you should return a real number, not a list containing a real number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, do a quick check of your log_likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-295.28069892]\n"
     ]
    }
   ],
   "source": [
    "# VERIFY - The value should be equal to -295.2806989185367.\n",
    "print(log_likelihood(X_train, y_train, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent\n",
    "Now write the code to perform gradient ascent.  You will use the update rule from the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write the gradient ascent function \n",
    "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
    "    # We assume X has been augmented with a column of ones\n",
    "    i=0\n",
    "    # Initiating list to store values of log-likelihood after 100 iterations \n",
    "    log_likelihood_values = []\n",
    "    # Initialize w to be a zero vector of shape x_train.shape[1],1\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    for i in range(num_iters): \n",
    "\n",
    "     \n",
    "    \n",
    "   \n",
    "    \n",
    "     # Initialize N to the number of training examples\n",
    "     N = X.shape[0] \n",
    "    \n",
    "     # Gradient Ascent - local optimization technique\n",
    "     ## TODO\n",
    "    \n",
    "     \n",
    "\n",
    "     \n",
    "         \n",
    "\n",
    "     \n",
    "     ##    \n",
    "     # Every 100 iterations, store the log_likelihood for the current w\n",
    "     if (i % 100) == 0:\n",
    "            curr_log_likelihood =log_likelihood(X,y,w)\n",
    "            log_likelihood_values.append(curr_log_likelihood)\n",
    "            # On your own, monitor the learning process, print the iteration number, the log-likelihood, ...\n",
    "\n",
    "\n",
    "     X_t=np.transpose(X)\n",
    "     g=y-hypothesis(X,w)\n",
    "\n",
    "     ee=np.matmul(X_t,g)\n",
    "     ee=(learning_rate/N)*ee\n",
    "     w=w+ee       \n",
    "        \n",
    "    return w, log_likelihood_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After completing the code above, run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1056701 ]\n",
      " [-0.03553097]\n",
      " [-0.0536691 ]\n",
      " [ 0.17221516]\n",
      " [-0.31116366]\n",
      " [-0.45434972]\n",
      " [ 2.75131896]\n",
      " [-1.30570826]\n",
      " [-3.03314808]\n",
      " [ 1.16803283]\n",
      " [-0.85636742]\n",
      " [-3.72793978]\n",
      " [ 0.72266145]\n",
      " [-0.80953786]\n",
      " [-2.47051644]\n",
      " [-0.40297532]\n",
      " [ 0.53078335]\n",
      " [ 0.03099043]\n",
      " [-1.27545053]\n",
      " [ 1.31001975]\n",
      " [ 2.1018566 ]\n",
      " [-1.64716699]\n",
      " [-2.84431473]\n",
      " [-0.20035616]\n",
      " [-1.77971341]\n",
      " [-0.16157425]\n",
      " [ 0.73553297]\n",
      " [-2.45713669]\n",
      " [-1.31520556]\n",
      " [-2.99931319]\n",
      " [-0.38232592]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "num_iters = 5000 # The number of iteratins to run the gradient ascent algorithm\n",
    "\n",
    "w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
    "\n",
    "print(w)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Log-Likelihood v/s Number of Iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
    "iters = np.array(range(0,num_iters,100))\n",
    "plt.plot(iters,log_likelihood_values,'.-',color='green')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title(\"Log-Likelihood vs Number of Iterations.\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the likelihood increasing as number of Iterations increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluating your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yhat=hypothesis(X_test,w)\n",
    "\n",
    "for i in range(len(yhat)):\n",
    "    if(yhat[i]>0.5):\n",
    "        yhat[i]=1\n",
    "    else:\n",
    "        yhat[i]=0\n",
    "\n",
    "c=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==yhat[i]):\n",
    "        c=c+1\n",
    "    else:\n",
    "        c=c+0\n",
    "\n",
    "\n",
    "print('The accuracy  is :',((c/len(y_test)))*100,'%')\n",
    "print('The percentage error is :',((len(y_test)-c)/len(y_test))*100)\n",
    "\n",
    "\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==1 and yhat[i]==1):\n",
    "        tp=tp+1\n",
    "    elif(y_test[i]==1 and yhat[i]==0):\n",
    "        fn=fn+1\n",
    "    elif(y_test[i]==0 and yhat[i]==1):  \n",
    "        fp=fp+1\n",
    "    elif(y_test[i]==0 and yhat[i]==0):\n",
    "        tn=tn+1\n",
    "\n",
    "\n",
    "\n",
    "p=tp/(tp+fp)\n",
    "r=tp/(tp+fn)\n",
    "\n",
    "print('The precision and recall of the classifier is ',p,'  ',r,' respectively.')\n",
    "f1=(2*p*r)/(p+r)\n",
    "print('The f1 score is:',f1)\n",
    "co=np.array([[tp,fp],[fn,tn]])\n",
    "print('The confusion matrix is')\n",
    "print(co)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Experiment with different hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy  is : 98.6013986013986 %\n",
      "The percentage error is : 1.3986013986013985\n",
      "The precision and recall of the classifier is  0.9887640449438202    0.9887640449438202  respectively.\n",
      "The f1 score is: 0.9887640449438202\n",
      "The confusion matrix is\n",
      "[[88  1]\n",
      " [ 1 53]]\n",
      "The accuracy  is : 97.9020979020979 %\n",
      "The percentage error is : 2.097902097902098\n",
      "The precision and recall of the classifier is  0.9886363636363636    0.9775280898876404  respectively.\n",
      "The f1 score is: 0.983050847457627\n",
      "The confusion matrix is\n",
      "[[87  1]\n",
      " [ 2 53]]\n",
      "The accuracy  is : 97.9020979020979 %\n",
      "The percentage error is : 2.097902097902098\n",
      "The precision and recall of the classifier is  0.9886363636363636    0.9775280898876404  respectively.\n",
      "The f1 score is: 0.983050847457627\n",
      "The confusion matrix is\n",
      "[[87  1]\n",
      " [ 2 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n",
      "The accuracy  is : 97.2027972027972 %\n",
      "The percentage error is : 2.797202797202797\n",
      "The precision and recall of the classifier is  0.9885057471264368    0.9662921348314607  respectively.\n",
      "The f1 score is: 0.9772727272727273\n",
      "The confusion matrix is\n",
      "[[86  1]\n",
      " [ 3 53]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.05\n",
    "i=0\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    num_iters = 5000 \n",
    "\n",
    "    w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
    "    learning_rate=learning_rate+0.1\n",
    "    \n",
    "    yhat=hypothesis(X_test,w)\n",
    "\n",
    "    for i in range(len(yhat)):\n",
    "       if(yhat[i]>0.5):\n",
    "            yhat[i]=1\n",
    "       else:\n",
    "          yhat[i]=0\n",
    "\n",
    "    c=0\n",
    "    for i in range(len(y_test)):\n",
    "      if(y_test[i]==yhat[i]):\n",
    "        c=c+1\n",
    "      else:\n",
    "        c=c+0\n",
    "\n",
    "\n",
    "    print('The accuracy  is :',((c/len(y_test)))*100,'%')\n",
    "    print('The percentage error is :',((len(y_test)-c)/len(y_test))*100)\n",
    "\n",
    "\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "      if(y_test[i]==1 and yhat[i]==1):\n",
    "          tp=tp+1\n",
    "      elif(y_test[i]==1 and yhat[i]==0):\n",
    "        fn=fn+1\n",
    "      elif(y_test[i]==0 and yhat[i]==1):  \n",
    "        fp=fp+1\n",
    "      elif(y_test[i]==0 and yhat[i]==0):\n",
    "        tn=tn+1\n",
    "\n",
    "\n",
    "\n",
    "    p=tp/(tp+fp)\n",
    "    r=tp/(tp+fn)\n",
    "\n",
    "    print('The precision and recall of the classifier is ',p,'  ',r,' respectively.')\n",
    "    f1=(2*p*r)/(p+r)\n",
    "    print('The f1 score is:',f1)\n",
    "    co=np.array([[tp,fp],[fn,tn]])\n",
    "    print('The confusion matrix is')\n",
    "    print(co)\n",
    "\n",
    "        \n",
    "\n",
    "#with a lower learning rate of 0.05 a higher accuracy precision recall and f1 score was obtained since the weights converge better \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
